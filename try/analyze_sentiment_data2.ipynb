{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f09d1b43",
   "metadata": {},
   "source": [
    "# Sentiment Data Analysis - Parquet Files\n",
    "\n",
    "This notebook demonstrates how to read and analyze sentiment data stored in partitioned Parquet files located in `/data/sentiment/`.\n",
    "\n",
    "## Data Structure\n",
    "- **Location**: `/data/sentiment/`\n",
    "- **Partitioning**: Hive-style partitioning by `ticker`, `year`, `month`, `day`\n",
    "- **Format**: Parquet with Snappy compression\n",
    "- **Schema**: 8 columns containing tweet/post sentiment analysis results\n",
    "\n",
    "## Analysis Goals\n",
    "1. Explore the partitioned data structure\n",
    "2. Perform aggregations on sentiment scores\n",
    "3. Analyze trends across tickers and time periods\n",
    "4. Visualize sentiment distributions\n",
    "5. Compare different data sources (Twitter vs Reddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e46511f",
   "metadata": {},
   "source": [
    "## 1. Setup - Import Required Libraries\n",
    "\n",
    "Import necessary libraries for data processing, analysis, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34e545b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# PySpark for distributed data processing\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m col, count, avg, \u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mmax\u001b[39m, desc, from_unixtime, to_date\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mround\u001b[39m \u001b[38;5;28;01mas\u001b[39;00m spark_round\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "# PySpark for distributed data processing\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, avg, min, max, desc, from_unixtime, to_date\n",
    "from pyspark.sql.functions import round as spark_round\n",
    "\n",
    "# Pandas for data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# PyArrow for low-level Parquet operations\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# File operations\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7debce",
   "metadata": {},
   "source": [
    "## 2. Initialize PySpark Environment\n",
    "\n",
    "Create a SparkSession optimized for reading partitioned Parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623dc067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session with optimized configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SentimentDataAnalysis\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.parquet.enableVectorizedReader\", \"true\") \\\n",
    "    .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n",
    "    .config(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce verbosity\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"✓ Spark {spark.version} session created successfully\")\n",
    "print(f\"  Master: {spark.sparkContext.master}\")\n",
    "print(f\"  App Name: {spark.sparkContext.appName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1877caf",
   "metadata": {},
   "source": [
    "## 3. Read Partitioned Parquet Data\n",
    "\n",
    "Load all sentiment data from `/data/sentiment/`. Spark automatically discovers the partition structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07580deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all partitioned Parquet files\n",
    "data_path = \"/data/sentiment\"\n",
    "df = spark.read.parquet(data_path)\n",
    "\n",
    "# Cache for performance (data will be reused multiple times)\n",
    "df.cache()\n",
    "\n",
    "print(f\"✓ Loaded data from: {data_path}\")\n",
    "print(f\"  Total records: {df.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7de1299",
   "metadata": {},
   "source": [
    "### Display Schema\n",
    "\n",
    "The schema includes 8 data columns plus 4 partition columns (ticker, year, month, day)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776f250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display complete schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30ab831",
   "metadata": {},
   "source": [
    "### Sample Data\n",
    "\n",
    "Display first 20 records to understand the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cece5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample records\n",
    "df.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8c883b",
   "metadata": {},
   "source": [
    "## 4. Explore Partitions\n",
    "\n",
    "Analyze the partition structure to understand data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13c5e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show distinct values for partition columns\n",
    "print(\"=== Partition Column Values ===\\n\")\n",
    "\n",
    "print(\"Tickers:\")\n",
    "df.select(\"ticker\").distinct().orderBy(\"ticker\").show(truncate=False)\n",
    "\n",
    "print(\"\\nYears:\")\n",
    "df.select(\"year\").distinct().orderBy(\"year\").show(truncate=False)\n",
    "\n",
    "print(\"\\nMonths:\")\n",
    "df.select(\"month\").distinct().orderBy(\"month\").show(truncate=False)\n",
    "\n",
    "print(\"\\nDays:\")\n",
    "df.select(\"day\").distinct().orderBy(\"day\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad4ae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count records per ticker\n",
    "print(\"=== Records per Ticker ===\\n\")\n",
    "df.groupBy(\"ticker\") \\\n",
    "    .agg(count(\"*\").alias(\"record_count\")) \\\n",
    "    .orderBy(desc(\"record_count\")) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74ec4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count records per day\n",
    "print(\"=== Records per Day ===\\n\")\n",
    "df.groupBy(\"year\", \"month\", \"day\") \\\n",
    "    .agg(count(\"*\").alias(\"record_count\")) \\\n",
    "    .orderBy(\"year\", \"month\", \"day\") \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be6de3c",
   "metadata": {},
   "source": [
    "## 5. Efficient Filtering with Partition Pruning\n",
    "\n",
    "Demonstrate how to filter data efficiently using partition columns. Spark will skip reading irrelevant partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a2d15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for AAPL in January 2026\n",
    "aapl_jan = df.filter(\n",
    "    (col(\"ticker\") == \"AAPL\") &\n",
    "    (col(\"year\") == 2026) &\n",
    "    (col(\"month\") == 1)\n",
    ")\n",
    "\n",
    "print(f\"AAPL records in January 2026: {aapl_jan.count()}\")\n",
    "aapl_jan.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8a1d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for specific date (January 8, 2026)\n",
    "jan8_data = df.filter(\n",
    "    (col(\"year\") == 2026) &\n",
    "    (col(\"month\") == 1) &\n",
    "    (col(\"day\") == 8)\n",
    ")\n",
    "\n",
    "print(f\"Records on January 8, 2026: {jan8_data.count()}\")\n",
    "\n",
    "# Show query execution plan to see partition pruning\n",
    "print(\"\\n=== Query Execution Plan (shows partition pruning) ===\")\n",
    "jan8_data.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba685f6",
   "metadata": {},
   "source": [
    "## 6. Sentiment Aggregations\n",
    "\n",
    "Perform various aggregations to understand sentiment patterns across tickers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80d19e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment distribution by ticker\n",
    "print(\"=== Sentiment Distribution by Ticker ===\\n\")\n",
    "sentiment_by_ticker = df.groupBy(\"ticker\", \"sentiment\") \\\n",
    "    .agg(count(\"*\").alias(\"count\")) \\\n",
    "    .orderBy(\"ticker\", \"sentiment\")\n",
    "\n",
    "sentiment_by_ticker.show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c93807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average sentiment score by ticker\n",
    "print(\"=== Average Sentiment Score by Ticker ===\\n\")\n",
    "avg_scores = df.groupBy(\"ticker\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_records\"),\n",
    "        spark_round(avg(\"score\"), 4).alias(\"avg_score\"),\n",
    "        spark_round(min(\"score\"), 4).alias(\"min_score\"),\n",
    "        spark_round(max(\"score\"), 4).alias(\"max_score\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"avg_score\"))\n",
    "\n",
    "avg_scores.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8796a6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sentiment percentages per ticker\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "\n",
    "# Count by ticker and sentiment\n",
    "counts = df.groupBy(\"ticker\", \"sentiment\").agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "# Calculate total per ticker\n",
    "window = Window.partitionBy(\"ticker\")\n",
    "sentiment_pct = counts.withColumn(\"total\", spark_sum(\"count\").over(window)) \\\n",
    "    .withColumn(\"percentage\", spark_round((col(\"count\") / col(\"total\")) * 100, 2)) \\\n",
    "    .orderBy(\"ticker\", \"sentiment\")\n",
    "\n",
    "print(\"=== Sentiment Percentages by Ticker ===\\n\")\n",
    "sentiment_pct.show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740a5195",
   "metadata": {},
   "source": [
    "## 7. Time-Series Analysis\n",
    "\n",
    "Convert timestamps and analyze sentiment trends over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a48290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert processedAt timestamp (milliseconds) to readable date\n",
    "df_with_date = df.withColumn(\n",
    "    \"processed_date\",\n",
    "    to_date(from_unixtime(col(\"processedAt\") / 1000))\n",
    ")\n",
    "\n",
    "# Show sample with converted dates\n",
    "print(\"=== Sample Data with Converted Dates ===\\n\")\n",
    "df_with_date.select(\"ticker\", \"sentiment\", \"score\", \"processedAt\", \"processed_date\") \\\n",
    "    .show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c892490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily sentiment trends by ticker\n",
    "print(\"=== Daily Sentiment Trends ===\\n\")\n",
    "daily_trends = df_with_date.groupBy(\"ticker\", \"processed_date\", \"sentiment\") \\\n",
    "    .agg(count(\"*\").alias(\"count\")) \\\n",
    "    .orderBy(\"processed_date\", \"ticker\", \"sentiment\")\n",
    "\n",
    "daily_trends.show(30, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4722a63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily average sentiment scores by ticker\n",
    "print(\"=== Daily Average Sentiment Scores ===\\n\")\n",
    "daily_avg = df_with_date.groupBy(\"ticker\", \"processed_date\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"record_count\"),\n",
    "        spark_round(avg(\"score\"), 4).alias(\"avg_score\")\n",
    "    ) \\\n",
    "    .orderBy(\"processed_date\", \"ticker\")\n",
    "\n",
    "daily_avg.show(30, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66161937",
   "metadata": {},
   "source": [
    "## 8. Source Platform Analysis\n",
    "\n",
    "Compare sentiment across different source platforms (Twitter vs Reddit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fee379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count records by source platform\n",
    "print(\"=== Records by Source Platform ===\\n\")\n",
    "df.groupBy(\"source\") \\\n",
    "    .agg(count(\"*\").alias(\"count\")) \\\n",
    "    .orderBy(desc(\"count\")) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df64e7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare average scores between sources\n",
    "print(\"=== Average Sentiment Score by Source ===\\n\")\n",
    "df.groupBy(\"source\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"count\"),\n",
    "        spark_round(avg(\"score\"), 4).alias(\"avg_score\")\n",
    "    ) \\\n",
    "    .orderBy(\"source\") \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8283fe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top publishers by record count\n",
    "print(\"=== Top 10 Publishers ===\\n\")\n",
    "df.groupBy(\"publisher\") \\\n",
    "    .agg(count(\"*\").alias(\"post_count\")) \\\n",
    "    .orderBy(desc(\"post_count\")) \\\n",
    "    .show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963b694b",
   "metadata": {},
   "source": [
    "## 9. Visualizations\n",
    "\n",
    "Create charts to visualize sentiment patterns. Convert Spark DataFrames to Pandas for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2b8b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas for visualization\n",
    "pdf = df.toPandas()\n",
    "\n",
    "print(f\"✓ Converted {len(pdf):,} records to Pandas DataFrame\")\n",
    "print(f\"  Memory usage: {pdf.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cccda12",
   "metadata": {},
   "source": [
    "### Sentiment Counts by Ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836b5f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "ticker_counts = pdf['ticker'].value_counts().sort_index()\n",
    "ticker_counts.plot(kind='bar', color='steelblue', edgecolor='black')\n",
    "plt.title('Sentiment Records by Ticker', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Ticker', fontsize=12)\n",
    "plt.ylabel('Number of Records', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f766575d",
   "metadata": {},
   "source": [
    "### Distribution of Sentiment Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54244ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(pdf['score'], bins=50, color='teal', edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribution of Sentiment Scores', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Sentiment Score', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.axvline(pdf['score'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {pdf[\"score\"].mean():.4f}')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e1023c",
   "metadata": {},
   "source": [
    "### Score Distribution by Ticker (Box Plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbc2f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "pdf_sorted = pdf.sort_values('ticker')\n",
    "sns.boxplot(data=pdf_sorted, x='ticker', y='score', palette='Set2')\n",
    "plt.title('Sentiment Score Distribution by Ticker', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Ticker', fontsize=12)\n",
    "plt.ylabel('Sentiment Score', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75232e7b",
   "metadata": {},
   "source": [
    "### Sentiment Categories Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ff72e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sentiment_colors = {'POSITIVE': 'green', 'NEGATIVE': 'red', 'NEUTRAL': 'gray'}\n",
    "sentiment_counts = pdf['sentiment'].value_counts()\n",
    "ax = sentiment_counts.plot(kind='bar', color=[sentiment_colors.get(x, 'blue') for x in sentiment_counts.index], edgecolor='black')\n",
    "plt.title('Sentiment Categories Distribution', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Sentiment', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(sentiment_counts):\n",
    "    ax.text(i, v + 0.5, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f01396",
   "metadata": {},
   "source": [
    "### Records by Source Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d67a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "source_counts = pdf['source'].value_counts()\n",
    "colors = ['#1DA1F2', '#FF4500']  # Twitter blue, Reddit orange\n",
    "ax = source_counts.plot(kind='bar', color=colors[:len(source_counts)], edgecolor='black')\n",
    "plt.title('Records by Source Platform', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Source', fontsize=12)\n",
    "plt.ylabel('Number of Records', fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(source_counts):\n",
    "    ax.text(i, v + 0.5, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f09207",
   "metadata": {},
   "source": [
    "### Time Series: Average Daily Sentiment Score by Ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f9cc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert daily average data to Pandas\n",
    "daily_avg_pdf = daily_avg.toPandas()\n",
    "daily_avg_pdf['processed_date'] = pd.to_datetime(daily_avg_pdf['processed_date'])\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "for ticker in daily_avg_pdf['ticker'].unique():\n",
    "    ticker_data = daily_avg_pdf[daily_avg_pdf['ticker'] == ticker]\n",
    "    plt.plot(ticker_data['processed_date'], ticker_data['avg_score'], \n",
    "             marker='o', label=ticker, linewidth=2)\n",
    "\n",
    "plt.title('Average Daily Sentiment Score by Ticker', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Average Sentiment Score', fontsize=12)\n",
    "plt.legend(title='Ticker', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99af651f",
   "metadata": {},
   "source": [
    "## 10. Alternative Reading Methods\n",
    "\n",
    "Demonstrate other ways to read Parquet files depending on your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07999093",
   "metadata": {},
   "source": [
    "### Method A: Using Pandas with Glob Pattern\n",
    "\n",
    "Best for analyzing a single ticker or smaller datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add42666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all AAPL parquet files using glob\n",
    "aapl_pattern = \"/data/sentiment/ticker=AAPL/**/*.parquet\"\n",
    "aapl_files = glob.glob(aapl_pattern, recursive=True)\n",
    "\n",
    "print(f\"Found {len(aapl_files)} parquet files for AAPL:\")\n",
    "for f in aapl_files:\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "# Read and concatenate all files\n",
    "if aapl_files:\n",
    "    aapl_df = pd.concat([pd.read_parquet(f) for f in aapl_files], ignore_index=True)\n",
    "    \n",
    "    print(f\"\\nLoaded {len(aapl_df)} AAPL records\")\n",
    "    print(\"\\nBasic Statistics:\")\n",
    "    print(aapl_df['score'].describe())\n",
    "    \n",
    "    print(\"\\nSentiment Distribution:\")\n",
    "    print(aapl_df['sentiment'].value_counts())\n",
    "else:\n",
    "    print(\"No AAPL files found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fb6db2",
   "metadata": {},
   "source": [
    "### Method B: Using PyArrow for Schema Inspection\n",
    "\n",
    "Best for examining file structure and metadata without loading all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fdfefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a sample parquet file\n",
    "sample_files = glob.glob(\"/data/sentiment/ticker=AAPL/**/*.parquet\", recursive=True)\n",
    "\n",
    "if sample_files:\n",
    "    sample_file = sample_files[0]\n",
    "    print(f\"Inspecting: {sample_file}\\n\")\n",
    "    \n",
    "    # Read file with PyArrow\n",
    "    parquet_file = pq.ParquetFile(sample_file)\n",
    "    \n",
    "    print(\"=== Schema ===\")\n",
    "    print(parquet_file.schema)\n",
    "    \n",
    "    print(\"\\n=== Metadata ===\")\n",
    "    print(f\"Number of rows: {parquet_file.metadata.num_rows}\")\n",
    "    print(f\"Number of row groups: {parquet_file.metadata.num_row_groups}\")\n",
    "    print(f\"Created by: {parquet_file.metadata.created_by}\")\n",
    "    \n",
    "    print(\"\\n=== Row Group 0 Details ===\")\n",
    "    rg = parquet_file.metadata.row_group(0)\n",
    "    print(f\"Number of rows: {rg.num_rows}\")\n",
    "    print(f\"Total byte size: {rg.total_byte_size:,} bytes\")\n",
    "    \n",
    "    # Read as PyArrow Table\n",
    "    table = pq.read_table(sample_file)\n",
    "    print(f\"\\n=== Table Info ===\")\n",
    "    print(f\"Shape: {table.num_rows} rows x {table.num_columns} columns\")\n",
    "    print(f\"Column names: {table.column_names}\")\n",
    "    \n",
    "    # Convert to Pandas and show sample\n",
    "    sample_df = table.to_pandas()\n",
    "    print(\"\\n=== Sample Data ===\")\n",
    "    print(sample_df.head())\n",
    "else:\n",
    "    print(\"No parquet files found for inspection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b25459",
   "metadata": {},
   "source": [
    "## 11. Export Examples\n",
    "\n",
    "Export aggregated results for further processing or reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d410f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export average scores to CSV\n",
    "output_dir = \"/investpulse.net/try/output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "csv_path = f\"{output_dir}/sentiment_summary.csv\"\n",
    "avg_scores.toPandas().to_csv(csv_path, index=False)\n",
    "print(f\"✓ Exported summary to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4583eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write filtered data back to Parquet (example: only positive sentiment)\n",
    "positive_sentiment = df.filter(col(\"sentiment\") == \"POSITIVE\")\n",
    "\n",
    "parquet_output = f\"{output_dir}/positive_sentiment\"\n",
    "positive_sentiment.write.mode(\"overwrite\").parquet(parquet_output)\n",
    "\n",
    "print(f\"✓ Exported {positive_sentiment.count():,} positive sentiment records to: {parquet_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942c9cbe",
   "metadata": {},
   "source": [
    "## 12. Summary and Cleanup\n",
    "\n",
    "Key findings and cleanup operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2c60cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics\n",
    "total_records = df.count()\n",
    "total_tickers = df.select(\"ticker\").distinct().count()\n",
    "date_range = df.agg(min(\"day\").alias(\"min_day\"), max(\"day\").alias(\"max_day\")).collect()[0]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SENTIMENT DATA ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total Records: {total_records:,}\")\n",
    "print(f\"Unique Tickers: {total_tickers}\")\n",
    "print(f\"Date Range: Day {date_range['min_day']} to Day {date_range['max_day']}\")\n",
    "print(f\"Data Path: {data_path}\")\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"- Sentiment data is partitioned by ticker/year/month/day\")\n",
    "print(\"- Multiple sources: Twitter and Reddit\")\n",
    "print(\"- Scores range from -1.0 (negative) to 1.0 (positive)\")\n",
    "print(\"- Data includes timestamps for both processing and original post times\")\n",
    "print(\"\\nFiles exported to:\", output_dir)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c595cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()\n",
    "print(\"\\n✓ Spark session stopped successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
