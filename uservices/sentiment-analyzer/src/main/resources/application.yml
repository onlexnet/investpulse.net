spring:
  application:
    name: sentiment-analyzer
  kafka:
    bootstrap-servers: localhost:9092
    consumer:
      group-id: sentiment-analyzer-group
      auto-offset-reset: earliest
      # Manual commit mode: offsets committed only after Parquet write succeeds
      # This ensures at-least-once delivery guarantee for file persistence
      enable-auto-commit: false
      # Reduced from default 500 to match Parquet rotation threshold (1000)
      # Allows for approximately 2 batches per file, balancing poll efficiency
      # with file granularity. Adjust if rotation settings change.
      max-poll-records: 500
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
      properties:
        # Delegate actual deserialization to JsonDeserializer
        spring.deserializer.value.delegate.class: org.springframework.kafka.support.serializer.JsonDeserializer
        # Trust DTO package for payload types
        spring.json.trusted.packages: "net.investpulse.common.dto"
        # Use type headers added by producers (default true for JsonSerializer)
        spring.json.use.type.headers: true
        # Increase poll interval to prevent rebalancing during slow Parquet writes
        # Default is 300000ms (5min), increased to account for file rotation delays
        max.poll.interval.ms: 600000
    listener:
      # Required for manual offset acknowledgment in @KafkaListener methods
      # Enables Acknowledgment parameter injection for programmatic offset commits
      ack-mode: MANUAL
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
  jackson:
    serialization:
      write-dates-as-timestamps: true
      # Format: timestamp in milliseconds for Instant
      write-date-timestamps-as-nanoseconds: false
    deserialization:
      read-dates-as-timestamps: true
      read-date-timestamps-as-nanoseconds: false

# Parquet persistence configuration for Spark-optimized analytics
sentiment:
  # Local timezone for Parquet partition date calculation
  # Posts from X (Twitter) and Reddit are stored with original UTC timestamps,
  # but partitioned by local date for business-day alignment
  # Examples: "UTC", "America/New_York", "Europe/London", "Asia/Tokyo"
  timezone: UTC
  parquet:
    # Base directory for partitioned Parquet files (auto-created if missing)
    # Partition structure: {base-path}/ticker={SYMBOL}/year={YYYY}/month={MM}/day={DD}/
    # Partition dates are calculated from original source timestamps in local timezone
    base-path: /data/sentiment
    # Maximum records per Parquet file before rotation (balances file size vs count)
    # Smaller values = more files, faster queries on specific time ranges
    # Larger values = fewer files, better compression ratios
    max-records-per-file: 1000
    # Bounded queue capacity for async writes (drop-and-log policy when full)
    # Size based on expected throughput: 10000 records â‰ˆ 10-20 seconds buffer at 500-1000 msg/sec
    queue-capacity: 10000
